{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ACOUSTIC MONITORING OF ECOSYSTEMS\n",
    "\n",
    "Ecosystems contain vast amounts of data that tell a lot about what is happening in them. Using acoustic monitoring we can monitor wildlife remotely and continously. Animals produce sounds that we can use to identify them without having to observe them physically. Birds, especially, vocalize a lot and we can use them in studying our ecosystems. In this notebook, we will see how we achieve acoustic classification of birds. We will classify three species of birds common in Kenya. The species are:\n",
    "1. Hartlaub's turaco\n",
    "2. Tropical boubou\n",
    "3. Grey-backed camaroptera\n",
    "\n",
    "We will use local audio data and data from [Xeno-canto](https://www.xeno-canto.org/) for this task. \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import necessary libraries\r\n",
    "import numpy as np\r\n",
    "import librosa \r\n",
    "import os\r\n",
    "import librosa.display\r\n",
    "import IPython.display\r\n",
    "import csv\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import warnings\r\n",
    "import configparser\r\n",
    "import random\r\n",
    "from tqdm import tqdm\r\n",
    "import soundfile as sf\r\n",
    "from scipy.ndimage import binary_dilation, binary_erosion"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "warnings.filterwarnings('ignore')\r\n",
    "\r\n",
    "# Get parameters from configuration file\r\n",
    "config = configparser.ConfigParser()\r\n",
    "config.read('files/parameters-birds.ini')\r\n",
    "\r\n",
    "win_len_ms = int(config['audio']['win_len_ms'])\r\n",
    "overlap = float(config['audio']['overlap'])\r\n",
    "sampling_rate = int(config['audio']['sampling_rate'])\r\n",
    "duration = float(config['neural-net']['input_duration_s'])\r\n",
    "rnd_seed = int(config['neural-net']['seed'])\r\n",
    "num_mels = int(config['baseline']['num_mels'])\r\n",
    "\r\n",
    "\r\n",
    "# Derive audio processing values\r\n",
    "win_len = int((win_len_ms * sampling_rate) / 1000)\r\n",
    "hop_len = int(win_len * (1 - overlap))\r\n",
    "nfft = int(2 ** np.ceil(np.log2(win_len)))\r\n",
    "num_frame = int((0.5 * duration * sampling_rate) / hop_len)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load and play 3 seconds of sample recordings for the three species:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Hartlaub's turaco\r\n",
    "y, _ = librosa.load('audio-samples/bird-samples/hartlaub\\'s turaco.wav', sr=sampling_rate)\r\n",
    "IPython.display.Audio(y[: int(sampling_rate * 3)], rate=sampling_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Tropical boubou\r\n",
    "y, _ = librosa.load('audio-samples/bird-samples/tropical-boubou.wav', sr=sampling_rate)\r\n",
    "IPython.display.Audio(y[: int(sampling_rate * 3)], rate=sampling_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grey-backed camaropteraabs\r\n",
    "y, _ = librosa.load('audio-samples/bird-samples/grey-backed camaroptera.wav', sr=sampling_rate)\r\n",
    "IPython.display.Audio(y[: int(sampling_rate * 3)], rate=sampling_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The samples sound differently to our ears. Anyone with the experience of telling the three species from their sounds can easily identify them from listening to the samples we have just played. We intend to enable a digital computer to mimick this behaviour and identify different bird species from their sounds. This will be achieved using digital signal processing and machine learning.\n",
    "\n",
    "## Acoustic Classification of Birds\n",
    "Acoustic classification of birds is an interesting field that can find its use in ecosystems conservation, ornithology and also for bird watchers. Manual classification of acoustic data is cumbersome and so we opt for automatic acoustic classification. \n",
    "\n",
    "### Acoustic data collection\n",
    "To acheive automatic acoustic classification of birds, data is required to train machine learning models. I have developed a Raspberry Pi based acoustic sensor that is used for data collection.\n",
    "\n",
    "![Acoustic sensor](files/sensor-setup.png)\n",
    "![Power supply board](files/labeled-power-board.png)\n",
    "![Acoustic sensor deployment](files/sensor-deployed-&-labels.png)\n",
    "\n",
    "Automatic acoustic classification of birds is faced by the following challenges:\n",
    "1. Limited annotated data per species\n",
    "2. Background noise\n",
    "3. Variable length of recordings\n",
    "4. Multiple birds singing simultaneously\n",
    "To achieve reasonble results, we need to manipulate the data. This is acheived using data preprocessing and data augmentation.\n",
    "\n",
    "### Data preprocessing\n",
    "In this task, we separated audio files into signal into signal part where bird sounds are audible and noise part where there are no bird sounds (background noise may be present in this part). The separation of the audio file into signal and noise parts is done as described in **Sprengel, E., Jaggi, M., Kilcher, Y., & Hofmann, T. (2016). Audio based bird species identification using deep learning techniques**.\n",
    "\n",
    "#### Signal extraction\n",
    "This acheived by computing the spectrogram of an audio file and then selecting pixels that have a value greater than a given threshold and treating them as corresponding to signal.\n",
    "#### Noise extraction\n",
    "This was acheived by selecting the pixel values of the spectrogram that are less than a given threshold and treating them as corresponding to noise.\n",
    "\n",
    "![Segmentation](files/segmentation.png)\n",
    "\n",
    "### Data augmentation\n",
    "#### Audio-noise padding\n",
    "For our classification models, we need fixed size inputs. However, before and after signal and noise separation, some files have shorter lengths than the required threshold length. To ensure these files are of at least threshold-length long, we will pad them with noise. Padding a signal with noise helps in developing the model by exposing it to the noise it will encounter in the field. \n",
    "![Noise padding](files/noise-padding.png)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Features Extraction\n",
    "\n",
    "We will extract features from the recordings and use them as inputs to our machine learning models. We will use melspectrograms for this task. The melspectrograms are obtained by applying 40 mel filters to the spectrograms of the recordings. \n",
    "\n",
    "Let's compute the melspectrograms of the recordings of the three species we played at the beginning of this notebook:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mels_dir = 'sample-melspects/'\r\n",
    "if not os.path.exists(mels_dir):\r\n",
    "    os.mkdir(mels_dir)\r\n",
    "\r\n",
    "samples_dir = './audio-samples/bird-samples'\r\n",
    "files = os.listdir(samples_dir)\r\n",
    "\r\n",
    "for file in files:\r\n",
    "    y, _ = librosa.load(os.path.join(samples_dir, file), sr=sampling_rate)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    melspect = librosa.feature.melspectrogram(y[: int(10 * sampling_rate)],\r\n",
    "                                            sr=sampling_rate,\r\n",
    "                                            n_fft=nfft,\r\n",
    "                                            hop_length=hop_len,\r\n",
    "                                            win_length=win_len,\r\n",
    "                                            window='hamming',\r\n",
    "                                            n_mels=num_mels)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    np.save(mels_dir + file.split('.')[0] + '.npy', melspect)\r\n",
    "\r\n",
    "\r\n",
    "    plt.figure()\r\n",
    "    librosa.display.specshow(librosa.amplitude_to_db(melspect, ref=np.max),\r\n",
    "                            hop_length=hop_len,\r\n",
    "                            sr=sampling_rate,\r\n",
    "                            y_axis='mel',\r\n",
    "                            x_axis='time')\r\n",
    "    plt.title(file.split('.')[0], fontsize=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can visually observe that the melspectrograms of the three species are different. This is because of difference in frequency components of sounds of birds belonging to different species.  We will be using this uniqueness to identify different species of birds from their sound.\n",
    "\n",
    "\n",
    "\n",
    "### Features Generation\n",
    "\n",
    "The features that were fed to machine learning models are the mean and the variance of the frequency channels. We split the melspectrograms into chunks of equal length. Therefore, from one audio data, we obtain multiple features that we feed to machine learning models. This helps solve the problem of limited data per species and that of variable length of recordings.\n",
    "\n",
    "Let's load the melspectrograms of the three species and perform these operations:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "files = [file for file in os.listdir(mels_dir) if file.endswith('.npy')]\r\n",
    "\r\n",
    "for file in files:\r\n",
    "    melspect = np.load(os.path.join(mels_dir, file))\r\n",
    "    if melspect.shape[1] > 2 * num_frame + 1: #process audio block of length 3 seconds\r\n",
    "        indx = random.randint(num_frame, melspect.shape[1] - num_frame - 1)\r\n",
    "        current_feature = melspect[:, indx - num_frame: indx + num_frame + 1]\r\n",
    "        plt.figure()\r\n",
    "        plt.plot(np.mean(current_feature, axis=1), label='mean')\r\n",
    "        plt.plot(np.std(current_feature, axis=1),  label='std_deviation')\r\n",
    "        plt.xlabel('Channels')\r\n",
    "        plt.title(file.split('.')[0])\r\n",
    "        plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Models\n",
    "A baseline model can be defined as a simple model that provides reasonable results on a task and requires not much time and expertise to develop. Baseline models helps put the more complex models into context in terms of accuracy. The results obtained from a baseline model should guide us in making the choice of complex model to use. In this classification task, we will use `Linear Support Vector Classifier`, `Support Vector Classifier`, `Multilayer Perceptron Classifier` and `Random Forests Classifiers` as our baseline models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#import necessary libraries\r\n",
    "import pickle\r\n",
    "from keras import layers\r\n",
    "from sklearn.svm import SVC, LinearSVC\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\r\n",
    "import features_generation as fg\r\n",
    "import ipywidgets as wg\r\n",
    "from sklearn.svm import SVC, LinearSVC\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\r\n",
    "import time\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def all_summary_features(feature, filename, annotation_dict, num_frame):\r\n",
    "    \"\"\" Splits melspectrograms into chunks. \r\n",
    "    Args:feature- melspectrogram\r\n",
    "         filename- file name of the melspectrogram\r\n",
    "         annotation_dict- a dictionary containing file names and labels\r\n",
    "         fmean- mean of the entire melspectrograms dataset channels\r\n",
    "         fstd- standard deviation of the entire melspectrograms dataset channels\r\n",
    "         num_frame- number of frames.\r\n",
    "         \r\n",
    "    Returns an array of features and their labels\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    file_features = []\r\n",
    "    file_labels = []\r\n",
    "\r\n",
    "    if feature.shape[1] > 2 * num_frame + 1:\r\n",
    "\r\n",
    "        for indx in range(num_frame, feature.shape[1] - num_frame - 1, num_frame):\r\n",
    "\r\n",
    "            current_feature = feature[:, indx - num_frame: indx + num_frame + 1]\r\n",
    "\r\n",
    "\r\n",
    "            file_features.append(np.concatenate((np.mean(current_feature, axis=1),\r\n",
    "                                            np.std(current_feature, axis=1))))\r\n",
    "\r\n",
    "\r\n",
    "            file_labels.append(annotation_dict[filename])\r\n",
    "\r\n",
    "    return np.array(file_features), np.array(file_labels)\r\n",
    "\r\n",
    "\r\n",
    "def train_val_split(filelist, annotation_dict):\r\n",
    "    \"\"\"Returns training and validation files and labels\r\n",
    "    Args:filelist- list of melpectrograms paths\r\n",
    "         annotation_dict- a dictionary of features and their labels\"\"\"\r\n",
    "    \r\n",
    "    all_features = np.array([])\r\n",
    "    all_labels = np.array([])\r\n",
    "    \r\n",
    "    for filename in filelist:\r\n",
    "        file_labels = []\r\n",
    "\r\n",
    "        feature = np.load(filename)\r\n",
    "        feature = np.log(feature + 1e-8)\r\n",
    "        file_features, file_labels = all_summary_features(feature,\r\n",
    "                                                            filename,\r\n",
    "                                                            annotation_dict,\r\n",
    "                                                            num_frame)\r\n",
    "        \r\n",
    "        if all_features.size:\r\n",
    "            all_features = np.vstack((all_features, file_features))\r\n",
    "        else:\r\n",
    "            all_features = file_features\r\n",
    "        all_labels = np.concatenate((all_labels, file_labels))\r\n",
    "        \r\n",
    "    X_train, X_val, y_train, y_val = train_test_split(all_features, all_labels, test_size=0.3, random_state=42)\r\n",
    "    \r\n",
    "    return X_train, X_val, y_train, y_val   \r\n",
    "\r\n",
    "\r\n",
    "def train_models(clf, model, models_dir, species, X_train, X_val, y_train, y_val):\r\n",
    "    \"\"\"Trains a classifier\r\n",
    "    Args: clf- classifier\r\n",
    "          model- name of model\r\n",
    "          models_dir- directory to save models\r\n",
    "          species- list containing species names\r\n",
    "          X_train- training data\r\n",
    "          X_val- validation data\r\n",
    "          y_train- training labels\r\n",
    "          y_val- validation labels\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    \r\n",
    "    print('============================================'.center(os.get_terminal_size().columns))\r\n",
    "    print('Training an {} classifier'.format(model).center(os.get_terminal_size().columns))\r\n",
    "    print('\\n')\r\n",
    "\r\n",
    "    start = time.time()\r\n",
    "\r\n",
    "    clf.fit(X_train, y_train)\r\n",
    "\r\n",
    "    end = time.time()\r\n",
    "\r\n",
    "    path = os.path.join(models_dir, model + '.pickle') #path to save models\r\n",
    "    pickle.dump(clf, open(path, 'wb'))\r\n",
    "\r\n",
    "\r\n",
    "    species_f1_score = f1_score(y_val, clf.predict(X_val), average=None)\r\n",
    "    species_precision_score = precision_score(y_val, clf.predict(X_val), average=None)\r\n",
    "    species_recall_score = recall_score(y_val, clf.predict(X_val), average=None)\r\n",
    "    df_species_metrics = pd.DataFrame(list(zip(species,\r\n",
    "                                               species_precision_score,\r\n",
    "                                               species_recall_score,\r\n",
    "                                               species_f1_score)),\r\n",
    "                                      columns=['Species', 'Precision', 'Recall', 'F1 Score'])\r\n",
    "    df_species_metrics = df_species_metrics.sort_values(['F1 Score'], ascending=False)\r\n",
    "    df_species_metrics = df_species_metrics.reset_index(drop=True)\r\n",
    "\r\n",
    "    time_spent = round((end - start) / 60, 2)\r\n",
    "\r\n",
    "    print('Time spent is:', time_spent, 'minutes')\r\n",
    "\r\n",
    "    print('Mean F1 score:', np.mean(species_f1_score))\r\n",
    "    print(df_species_metrics.round(2))\r\n",
    "    print('\\n')\r\n",
    "    \r\n",
    "    \r\n",
    "def model_test(index):\r\n",
    "    \"\"\"Test trained models\r\n",
    "    Args:index- index of model in model list\r\n",
    "         \"\"\"\r\n",
    "    \r\n",
    "    models_dir = 'models'\r\n",
    "    \r\n",
    "    models = ['svm', 'rf', 'svm-rbf', 'mlp', 'null']\r\n",
    "    \r\n",
    "    model = models[index]\r\n",
    "    \r\n",
    "    if model != 'null':\r\n",
    "        #load a pretrained model and use it to predict a sample test data\r\n",
    "        tree = next(os.walk('audio-samples/birds-test-data/'))\r\n",
    "\r\n",
    "        dir_paths = [os.path.join(tree[0], sub_dir) for sub_dir in tree[1]]\r\n",
    "\r\n",
    "        filelist = []\r\n",
    "        for sub_dir in dir_paths:\r\n",
    "            files = os.listdir(sub_dir)\r\n",
    "            for file in files:\r\n",
    "                filelist.append(os.path.join(sub_dir, file).replace('\\\\', '/'))\r\n",
    "\r\n",
    "        file_indx = random.randint(0, len(filelist) - 1)\r\n",
    "        file = filelist[file_indx]\r\n",
    "\r\n",
    "        target = file.split('/')[2]\r\n",
    "\r\n",
    "        clf = pickle.load(open(os.path.join(models_dir, model + '.pickle'), 'rb'))\r\n",
    "\r\n",
    "        audio,_ = librosa.load(file, sr=sampling_rate)\r\n",
    "        \r\n",
    "        noise_dir = 'audio-samples/noise'\r\n",
    "        \r\n",
    "        feature = fg.features_extraction(audio,\r\n",
    "                                     nfft,\r\n",
    "                                     hop_len,\r\n",
    "                                     noise_dir,\r\n",
    "                                     sampling_rate,\r\n",
    "                                     duration,\r\n",
    "                                     win_len,\r\n",
    "                                     hop_len,\r\n",
    "                                     num_mels)\r\n",
    "\r\n",

    "        feature = np.log(feature + 1e-8)\r\n",
    "\r\n",
    "        features = np.concatenate((np.mean(feature, axis=1),\r\n",
    "                                                np.std(feature, axis=1))).reshape(1, -1)\r\n",
    "\r\n",
    "\r\n",
    "        species  = ['grey-backed camaroptera', 'hartlaub\\'s turaco', 'tropical boubou']\r\n",
    "\r\n",
    "\r\n",
    "        predicted = clf.predict(features)\r\n",
    "\r\n",
    "\r\n",
    "        indx = random.randint(0, len(predicted) - 1)\r\n",
    "\r\n",
    "        predicted = species[int(predicted[indx])]\r\n",
    "\r\n",
    "        print('Predicted species:',  predicted, '\\n', 'Expected species:', target)\r\n",
    "\r\n",
    "        \r\n",
    "def main():\r\n",
    "    \r\n",
    "    models_dir = './models'\r\n",
    "    annotation_csv = 'files/labels.csv'\r\n",
    "    mels_dir = './data/birds-melspectrograms/'\r\n",
    "    \r\n",
    "    if not os.path.exists(models_dir):\r\n",
    "        os.makedirs(models_dir)\r\n",
    "\r\n",
    "    labels = []\r\n",
    "    df_annotation = pd.read_csv(annotation_csv)\r\n",
    "\r\n",
    "    species = list(set(df_annotation['label']))\r\n",
    "    species.sort()\r\n",
    "    species_dict = dict(zip(species, range(len(species))))\r\n",
    "    species_dict\r\n",
    "    for file_species in list(df_annotation['label']):\r\n",
    "        labels.append(species_dict[file_species])   \r\n",
    "    filelist = list(df_annotation.name)\r\n",
    "    for indx, filename in enumerate(filelist):\r\n",
    "        filelist[indx] = os.path.join(mels_dir, filename)\r\n",
    "\r\n",
    "    annotation_dict = dict(zip(filelist, labels))\r\n",
    "    \r\n",
    "    df = pd.read_csv('files/models_params.csv')\r\n",
    "\r\n",
    "    models = ['svm', 'rf', 'svm-rbf', 'mlp']\r\n",
    "    \r\n",
    "    X_train, X_val, y_train, y_val = train_val_split(filelist, annotation_dict)\r\n",
    "    \r\n",
    "    \r\n",
    "    for model in models:\r\n",
    "        param = int(df.best_param[df[df['model'] == model].index.values].item())\r\n",
    "        \r\n",
    "        if model == 'svm':\r\n",
    "            clf = LinearSVC(C=param, random_state=0, tol=1e-5, multi_class='crammer_singer', max_iter=10000)\r\n",
    "            \r\n",
    "        elif model == 'svm-rbf':\r\n",
    "            clf = SVC(gamma='auto', C=param, probability=True)\r\n",
    "            \r\n",
    "        elif model == 'mlp':\r\n",
    "            clf = MLPClassifier(hidden_layer_sizes=param, random_state=1, max_iter=10000)\r\n",
    "        elif model == 'rf':\r\n",
    "            clf = RandomForestClassifier(n_estimators=param, max_depth=None, random_state=0)\r\n",
    "            \r\n",
    "             \r\n",
    "            \r\n",
    "        train_models(clf, model, models_dir, species, X_train, X_val, y_train, y_val)\r\n",
    "        \r\n",
    "        \r\n",
    "        \r\n",
    "    drop_down_display = [('Choose Model', -1), ('svm', 0), ('rf', 1), ('svm-rbf', 2), ('mlp', 3)]\r\n",
    "\r\n",
    "\r\n",
    "    drop_down = wg.Dropdown(options=drop_down_display,\r\n",
    "                               description='Models:',\r\n",
    "                               disabled=False)\r\n",
    "\r\n",
    "    wg.interact(model_test, index=drop_down)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\r\n",
    "    main()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
